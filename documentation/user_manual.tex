\documentclass[a4paper]{article} % you can use the same options as for article class

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Meta Information %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\def\muni{Masaryk University}
\def\uvt{Institute of Computer Science}
\def\street{Botanick\'{a} 68a}
\def\psc{602\,00 Brno}

\def\projectname{AIDA Framework}

\def\reportauthor{Martin Hus\'{a}k, Jaroslav Ka\v{s}ar, Milan \v{Z}iaran}
\def\reporttitle{User Manual \& Documentation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Included Packages %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[paper=a4paper,top=2.5cm,bottom=2.5cm,left=2.5cm,right=2.5cm,bottom=2.5cm,foot=1cm]{geometry}
\usepackage{cmap}       % cut and past support for iso-8859-2 characters
\usepackage[utf8]{inputenc}
\usepackage[default,osfigures,scale=0.5]{opensans}
\usepackage[T1]{fontenc}
\usepackage[hyphens]{url}
\usepackage{hyperref}
\usepackage[usenames, dvipsnames]{xcolor}
% \usepackage{lastpage}
\usepackage{indentfirst}
\usepackage{graphicx}
% \usepackage{booktabs}
% \usepackage{multirow}
\DeclareGraphicsExtensions{.pdf, .ps, .eps, .png}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Colors %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\definecolor{projectcolor}{RGB}{93,118,168}
\definecolor{uvtcolor}{RGB}{188,4,78}
\definecolor{textcolor}{RGB}{90,90,90}
\definecolor{notecolor}{RGB}{150, 150, 150}
\definecolor{emphcolor}{RGB}{93,118,168}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Settings for pdf output %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\hypersetup{%
pdftitle   = {\reporttitle},
pdfauthor  = {\reportauthor},
pdfsubject = {\projectname},
pdfkeywords= {},
%
unicode=true,                % czech in bookmarks
bookmarks=true,              % turn on creation of bookmarks
bookmarksopen=false,         % expand subsections in bookmark tab
bookmarksnumbered = true,    % chapter numbers in bookmarks
bookmarksopenlevel = 5,
%
pdfpagemode=UseOutlines,     % UseThumbs, UseOutlines (turn on bookmarks), FullScreen, None
pdfpagelayout=SinglePage,    % SinglePage, OneColumn, TwoColumnLeft, TwoColumnRight
pdfstartview=FitV,           % Fit, FitB, FitH, FitV
%
backref = false,
linkcolor = textcolor,
citecolor = textcolor,
urlcolor = textcolor,
colorlinks = true,           % on/off link boxes
hyperindex=true,
plainpages=false,            % fix duplicated pages when roman/arabic numbering is used
%
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Document formating %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\renewcommand{\sfdefault}{opensans}
\renewcommand{\familydefault}{\sfdefault}
\renewcommand{\normalsize}{\fontsize{12}{15}\selectfont\color{textcolor}}

\tolerance 9999 % big white space tolerance for better typesetting and text hyphenation

\def\verbatim@font{\color{textcolor}} % color of verbatim text

\makeatletter       % section title formating
\renewcommand\section{\@startsection {section}{1}{\z@}%
                   {-3.5ex \@plus -1ex \@minus -.2ex}%
                   {2.3ex \@plus.2ex}%
                   {\normalfont\sffamily\Large\bfseries\color{projectcolor}}}
\renewcommand\subsection{\@startsection{subsection}{2}{\z@}%
                   {-3.25ex\@plus -1ex \@minus -.2ex}%
                   {1.5ex \@plus .2ex}%
                   {\normalfont\sffamily\large\bfseries\color{projectcolor}}}
\renewcommand\subsubsection{\@startsection{subsubsection}{3}{\z@}%
                   {-3.25ex\@plus -1ex \@minus -.2ex}%
                   {1.5ex \@plus .2ex}%
                   {\normalfont\normalsize\sffamily\bfseries\color{projectcolor}}}
\makeatother

\newenvironment{itemize*}%
{\begin{itemize}%
    \setlength{\itemsep}{0pt}%
    \setlength{\parskip}{0pt}%
}{\end{itemize}}

\usepackage[pages=some,scale=1,angle=0,opacity=1]{background}
\newcommand\BackImage[2][scale=1]{%
\BgThispage
\backgroundsetup{
  contents={\includegraphics[#1]{#2}}
  }
}
\usepackage[absolute,overlay]{textpos}

% fancy verbatim and listings
\usepackage{fancyvrb}
\usepackage{listings}
\definecolor{listingbg}{RGB}{242,242,242}
\lstset{
	basicstyle=\footnotesize\ttfamily,
	captionpos=b,
	backgroundcolor=\color{listingbg},
	framesep=4pt,
	frame=single,
	breaklines=true,
	rulecolor=\color{listingbg},
	aboveskip=10pt,
	xleftmargin=1cm
}

\begin{document}

% do not indent first line in a paragraph
\setlength{\parindent}{0cm}

% title page
{
\BackImage[width=1.4\textwidth]{fig/titlepage_bg}
\pdfbookmark[1]{Title page}{pdf_first_page}

\vspace*{5cm}

\color{white}
  \begin{center}
  \vspace{2cm} {\Huge \projectname}

  \vspace{1cm} {\huge\bfseries\MakeUppercase{\reporttitle}}
  \end{center}

  \begin{textblock}{3}(1,10)
      \hyphenpenalty=10000 % prevent names from hyphenating
         \begin{tabular}{rl }
            Authors & \reportauthor \\
             & \\
            Contact address & \muni \\
             & \uvt \\
             & \street \\
             & \psc \\
          \end{tabular}
      \hyphenpenalty=0
  \end{textblock}
  \cleardoublepage
}

\normalfont

% content page

\pdfbookmark[1]{Content}{pdf_contents}

\setcounter{tocdepth}{3}

\tableofcontents

\vspace*{\fill}

\normalfont

\pdfbookmark[1]{Acknowledgement}{Acknowledgement}
\section*{Acknowledgment}

The authors of the AIDA framework are Martin Hus\'{a}k, Jaroslav Ka\v{s}ar, and Milan \v{Z}iaran. The authors would like to thank Michal Pav\'{u}k, V\'{i}t Rus\v{n}\'{a}k, and Jakub Kolman for the dashboard, and Petr Velan and Samuel \v{S}u\v{l}an for testing the framework.

% AIDA Framework integrates SPMF library\footnote{\url{http://www.philippe-fournier-viger.com/spmf/}} distributed under GPL v3 license.

There is a design paper describing the AIDA framework, see:
\begin{lstlisting}[]
Martin Husák and Jaroslav Kašpar. 2019. AIDA Framework: Real-Time Correlation and Prediction of Intrusion Detection Alerts. In Proceedings of the 14th International Conference on Availability, Reliability and Security (ARES '19). ACM, New York, NY, USA, Article 81, 8 pages. DOI: https://doi.org/10.1145/3339252.3340513
\end{lstlisting}

To cite the paper, we recommend using the following bibtex entry:
\begin{lstlisting}[]
@inproceedings{AIDAframework,
 author = {Hus\'{a}k, Martin and Ka\v{s}par, Jaroslav},
 title = {AIDA Framework: Real-Time Correlation and Prediction of Intrusion Detection Alerts},
 booktitle = {Proceedings of the 14th International Conference on Availability, Reliability and Security},
 series = {ARES '19},
 year = {2019},
 isbn = {978-1-4503-7164-3},
 location = {Canterbury, CA, United Kingdom},
 pages = {81:1--81:8},
 doi = {10.1145/3339252.3340513},
 publisher = {ACM},
 address = {New York, NY, USA}
}
\end{lstlisting}

The development of the framework and related research were supported by the Security Research Programme of the Czech Republic 2015 - 2020 (BV III / 1 VS) granted by the Ministry of the Interior of the Czech Republic under No. VI20162019029 The Sharing and analysis of security events in the Czech Republic. 
Further research was supported by ERDF ``CyberSecurity, CyberCrime and Critical Information Infrastructures Center of Excellence'' (No.CZ.02.1.01/0.0/0.0/16\_019/0000822).

\cleardoublepage

\section{Introduction}

AIDA is an analytical framework for processing intrusion detection alerts with a focus on alert correlation and predictive analytics. The framework contains components that filter, aggregate, and correlate the alerts, and predict future security events using the predictive rules distilled from historical records. The components are based on stream processing and use selected features of data mining (namely sequential rule mining) and complex event processing. The framework was designed to be deployed as an analytical component of an alert processing platform. Alternatively, it can be deployed locally for experimentations over datasets.

\includegraphics[width=\textwidth]{fig/aida_schema}

IDEA\footnote{\url{https://idea.cesnet.cz/}}

\begin{lstlisting}[]
OrganizationA.Honeypot1_Recon.Scanning_22,
OrganizationB.IDS1_Attempt.Login_22
==>
OrganizationA.IDS1_Attempt.Login_22
#SUPP: 0.0011 #CONF: 0.6111
\end{lstlisting}

\begin{lstlisting}[]
{
  "Format": "IDEA0",
  "ID":"f62537c2-77b8-49c7-a0a2-24c4b81b20f8",
  "CorrelID": [ % IDs of preceding alerts
        "3688762d-2efa-44a8-9ea5-34a57b3ae0c7",
        "ae6d9ac6-6389-407f-9d7e-58b9692c6eaa"
      ],
  "DetectTime": "2019-03-16T12:17:21.609+00:00",
  "Category": ["Attempt.Login"],
  "Confidence": "0.6111",
  "Description": "The source IP address follows a known pattern that is expected to continue with the event described in this message.",
  "Note": "OrganizationA.Honeypot1_Recon.Scanning_22, OrganizationB.IDS1_Attempt.Login_22 ==> OrganizationA.IDS1_Attempt.Login_22",
  "Source": [{"IP4": ["10.11.12.13"]}],
  "Target": [{"Port":[22]}],
  "Node": [
    { % Node referencing the AIDA Framework
      "Name": "OrganizationX.AIDA",
      "SW": "AIDA",
      "Type": ["Correlation", "Statistical"]
    },
    { % Node derived from the rule
      "Name": "OrganizationA.IDS1"
    }
  ]
}
\end{lstlisting}

\cleardoublepage


\section{User Guide}

\subsection{Quick Start using Command Line}

For a quick start, go to the provision directory and run Vagrant:

\begin{lstlisting}[]
cd provision
vagrant up
\end{lstlisting}

AIDA framework will start in few minutes. Then, send your data to the framework using the following command (you need to have netcat installed):

\begin{lstlisting}[]
nc localhost 4164 < path_to_file_with_your_data
\end{lstlisting}

If you do not have your own data, we recommend trying AIDA framework out with our dataset\footnote{\url{http://dx.doi.org/10.17632/p6tym3fghz.1}}. Download and unzip the main file in the datase (dataset.idea.zip) and use it in the command above.

\subsubsection*{Run data mining}

Trigger the data mining procedure (otherwise, it starts every 24 hours that you would have to wait):

\begin{lstlisting}[]
sudo systemctl start mining
\end{lstlisting}

Check the logs of the data mining component:

\begin{lstlisting}[]
sudo journalctl -u mining
\end{lstlisting}

\subsubsection*{Update rules}

Open the database with the mined rules:

\begin{lstlisting}[]
sqlite3 /var/aida/rules/rule.db
\end{lstlisting}

Check the rules in the database:

\begin{lstlisting}[]
select * from rule;
\end{lstlisting}

Activate all the rules so that they are used by the rule matching component:

\begin{lstlisting}[]
update rule set active=1;
\end{lstlisting}

Restart matching component to start matching activated rules:

\begin{lstlisting}[]
sudo systemctl restart matching
\end{lstlisting}

Send some more data into AIDA, they will be matched against the rules to predict upcoming events:

\begin{lstlisting}[]
nc localhost 4164 < path_to_file_with_your_data
\end{lstlisting}

\subsubsection*{Check outputs}

Predicted rules are saved in the root directory of this repository in `predictions.json` file.

You can also get the predictions directly from Kafka:

\begin{lstlisting}[]
`/opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic predictions --from-beginning`
\end{lstlisting}

\cleardoublepage

\subsection{Quick Start using AIDA Dashboard}

http://localhost:8000/admin/

admin
aida.admin

http://localhost:8080

Username: admin
Password: aida.admin

\includegraphics[width=0.75\textwidth]{fig/dashboard_login}

\includegraphics[width=0.75\textwidth]{fig/dashboard_overview}

\includegraphics[width=0.75\textwidth]{fig/dashboard_rules}

\includegraphics[width=0.75\textwidth]{fig/dashboard_spark}

\cleardoublepage

\section{Installation Instructions}

Simplest way how to install and use AIDA is by running vagrant box \texttt{vagrant up} from \texttt{provision} directory.
If you want to install AIDA on an already existing machine, you can follow the following instructions. There is a Quick Installation
section whereby you can install AIDA just by running Ansible playbook. Or you can install AIDA manually step by step from
the instructions in Manual Installation section.



\subsection{Prerequisites}

Installed Ubuntu 18.04.2 and AIDA repository downloaded or cloned into \texttt{/vagrant} directory. You can choose any
other directory, but we will refer the repository root as \texttt{/vagrant}. You should have also installed following dependencies.


\begin{itemize}
    \item Installed Java JDK version 1.8
    \item Installed Maven version 3.6
    \item Installed Python 2.7 as \texttt{python}
    \item Installed Python 3.6 as \texttt{python3}
    \item Installed Pip for python 2.7 as \texttt{pip}
    \item Installed Pip for python 3.6 as \texttt{pip3}
\end{itemize}

You can install the above prerequisites by running

\begin{lstlisting}
$ sudo apt-get update
$ sudo apt-get -y install openjdk-8-jdk maven python-pip python3-pip
$ sudo update-java-alternatives --set java-1.8.0-openjdk-amd64
\end{lstlisting}



\subsection{Quick Installation}

The simplest way how to install AIDA is by running Ansible playbook.

You have to have Ansible installed. You can do it by running following command.

\begin{lstlisting}
$ sudo apt-get -y install ansible
\end{lstlisting}

Then run ansible playbook.

\begin{lstlisting}
$ cd /vagrant/provision/ansible
$ ansible-playbook -i inventory-local.ini install-aida.yml
\end{lstlisting}

Note that the Ansible will also install Java 8, Python 3 and other packages listed in prerequisites section.

% TODO provide simple command to check status of all AIDA services
You can check that the main AIDA services are up and running by running.
\begin{lstlisting}
$ sudo systemctl status aida-input sanitization aggregation matching
\end{lstlisting}



\subsection{Manual Installation}

Installing AIDA on existing machine step by step.

We will describe here creating of new users for almost each AIDA service, but you can of course create just one user for
all AIDA services.


\subsubsection{Install Kafka}

We will install Kafka and Zookeper and run single kafka broker on the machine.

Create Kafka user
\begin{lstlisting}
$ sudo useradd -M kafka
\end{lstlisting}

Install Zookeper service
\begin{lstlisting}
$ sudo apt-get -y install zookeeperd
\end{lstlisting}

Download Kafka binary into \texttt{/opt/kafka}
\begin{lstlisting}
$ sudo su
$ mkdir /opt/kafka
$ cd /opt/kafka
$ wget https://archive.apache.org/dist/kafka/2.3.0/kafka_2.12-2.3.0.tgz -O kafka.tgz
$ tar -xzf kafka.tgz --strip-components=1
$ rm kafka.tgz
$ sudo chown -R kafka:kafka ./
\end{lstlisting}

Create systemd unit file for \texttt{kafka} service
\begin{lstlisting}
$ sudo cp /vagrant/provision/ansible/roles/kafka/templates/kafka.service.j2 /etc/systemd/system/kafka.service
\end{lstlisting}

Then you have to replace following placeholders in the newly created file \texttt{/etc/systemd/system/kafka.service}.

\begin{itemize}
\item \texttt{\{\{ kafka\_user \}\}} with \texttt{kafka} or other user you had chosen to use
\item \texttt{\{\{ kafka\_dir \}\}} with \texttt{/opt/kafka}
\end{itemize}

Create systemd unit file for \texttt{kafka-topic} services

\begin{lstlisting}
$ sudo cp /vagrant/provision/ansible/roles/kafka/templates/kafka-topic.service.j2 /etc/systemd/system/kafka-topic@.service
\end{lstlisting}

Then replace the same placeholders as in previous step in the newly created file.

Now you can start the \texttt{kafka} service by running

\begin{lstlisting}
$ sudo systemctl start kafka
\end{lstlisting}

And check that it's running.

\begin{lstlisting}
$ sudo systemctl status kafka
\end{lstlisting}

Also check that \texttt{kafka-topic} service is working by creating \texttt{input} topic.

\begin{lstlisting}
$ sudo systemctl start kafka-topic@input
$ sudo systemctl status kafka-topic@input
\end{lstlisting}


\subsubsection{Install Spark}

We will install and configure Spark so we are able to run Spark applications in local mode.

Create Spark user
\begin{lstlisting}
$ sudo useradd -M spark
\end{lstlisting}

Download Spark binary
\begin{lstlisting}
$ sudo su
$ mkdir -p /opt/spark/spark
$ cd /opt/spark/spark
$ wget https://archive.apache.org/dist/spark/spark-2.4.3/spark-2.4.3-bin-hadoop2.7.tgz -O spark.tgz
$ tar -xzf spark.tgz --strip-components=1
$ rm spark.tgz
$ wget https://search.maven.org/classic/remotecontent?filepath=org/apache/spark/spark-streaming-kafka-0-8-assembly_2.11/2.4.3/spark-streaming-kafka-0-8-assembly_2.11-2.4.3.jar -O /opt/spark/spark/jars/spark-streaming-kafka-0-8-assembly_2.11-2.4.3.jar
\end{lstlisting}

Prepare modules for Spark applications
\begin{lstlisting}
$ sudo su
$ mkdir -p /opt/spark/applications
$ cp -r /vagrant/commons/commons-python /opt/spark/applications/modules
$ pip3 install -r /opt/spark/applications/modules/requirements.txt
$ apt-get -y install zip
$ zip -j -r /opt/spark/applications{.zip,}
\end{lstlisting}

Prepare script for running Spark applications
\begin{lstlisting}
$ sudo su
$ cp /vagrant/provision/ansible/roles/spark/templates/run-application.sh.j2 /opt/spark/applications/run-application.sh
$ chmod +x /opt/spark/applications/run-application.sh
\end{lstlisting}

Now you have to replace placeholder \texttt{\{\{ spark\_home\_dir \}\}} with \texttt{/opt/spark/spark}
in the \texttt{run-application.sh} file.

Create configuration file
\begin{lstlisting}
$ sudo cp /vagrant/provision/ansible/roles/spark/templates/spark-defaults.conf.j2 /opt/spark/spark/conf/spark-defaults.conf
\end{lstlisting}

Replace placeholder \texttt{\{\{ spark\_modules\_zip \}\}} with \texttt{/opt/spark/applications/modules/modules.zip}
in the newly created configuration file. You can also increase the \texttt{spark.driver.memory} due to available
memory to be able to pass more data into AIDA.

Make \texttt{spark} user owner of the \texttt{/opt/spark} dir
\begin{lstlisting}
$ sudo chown -R spark:spark /opt/spark/
\end{lstlisting}


\subsubsection{Install commons of AIDA services}

Install Java commons modules for AIDA services.

%TODO sudo or not sudo
\begin{lstlisting}
$ cd /vagrant/commons/commons-idea
$ mvn clean install
$ cd /vagrant/commons/commons-mining
$ mvn clean install
\end{lstlisting}


\subsubsection{Install aida-input service}

Install \texttt{aida-input} service which will listen on port 4164 and will send all incoming events into AIDA.
\begin{lstlisting}
$ sudo cp /vagrant/provision/ansible/roles/aida-input/templates/aida-input.service.j2 /etc/systemd/system/aida-input.service
\end{lstlisting}

Replace following placeholders in the newly created file \texttt{/etc/systemd/system/aida-input.service}.
\begin{itemize}
\item \texttt{\{\{ kafka\_user \}\}} with \texttt{kafka} or other user you had chosen to use while installing Kafka
\item \texttt{\{\{ kafka\_dir \}\}} with \texttt{/opt/kafka}
\item \texttt{\{\{ aida\_input\_port \}\}} with \texttt{4164}
\end{itemize}

Run the \texttt{aida-input} service
\begin{lstlisting}
$ sudo systemctl start aida-input
\end{lstlisting}

Check that the service is running
\begin{lstlisting}
$ sudo systemctl status aida-input
\end{lstlisting}


\subsubsection{Install sanitization service}

Create sanitization user
\begin{lstlisting}
$ sudo useradd -M sanitization
\end{lstlisting}

Install sanitization
\begin{lstlisting}
$ sudo mkdir /opt/sanitization
$ cd /vagrant/services/sanitization
$ mvn clean package
$ cp ./target/*jar-with-dependencies.jar /opt/sanitization/sanitization.jar
$ sudo chown -R sanitization:sanitization /opt/sanitization
\end{lstlisting}

Create systemd unit file for \texttt{sanitization} service
\begin{lstlisting}
$ sudo cp /vagrant/provision/ansible/roles/sanitization/templates/sanitization.service.j2 /etc/systemd/system/sanitization.service
\end{lstlisting}

Then replace following placeholders in newly created file
\begin{itemize}
\item \texttt{\{\{ sanitization\_user \}\}} with \texttt{sanitization} or other user you had chosen
\item \texttt{\{\{ sanitization\_dir \}\}} with \texttt{/opt/sanitization}
\end{itemize}

Run the \texttt{sanitization} service
\begin{lstlisting}
$ sudo systemctl start sanitization
\end{lstlisting}

Check that the service is running
\begin{lstlisting}
$ sudo systemctl status sanitization
\end{lstlisting}


\subsubsection{Install aggregation service}

Put Aggregation Spark application into required directory

\begin{lstlisting}
$ sudo cp -r /vagrant/services/aggregation /opt/spark/applications/aggregation
$ sudo chown -R spark:spark /opt/spark/applications/aggregation
\end{lstlisting}

Install python dependencies
\begin{lstlisting}
$ sudo pip3 install -r /opt/spark/applications/aggregation/requirements.txt
\end{lstlisting}

Create systemd unit file for \texttt{aggregation} service
\begin{lstlisting}
$ sudo cp /vagrant/provision/ansible/roles/aggregation/templates/aggregation.service.j2 /etc/systemd/system/aggregation.service
\end{lstlisting}

Then replace following placeholders in newly created file
\begin{itemize}
\item \texttt{\{\{ spark\_user \}\}} with \texttt{spark}
\item \texttt{\{\{ spark\_run\_app\_script \}\}} with \texttt{/opt/spark/applications/run-application.sh}
\item \texttt{\{\{ aggregation\_install\_dir \}\}} with \texttt{/opt/spark/applications/aggregation}
\end{itemize}

Run the \texttt{aggregation} service
\begin{lstlisting}
$ sudo systemctl start aggregation
\end{lstlisting}

Check that the service is running
\begin{lstlisting}
$ sudo systemctl status aggregation
\end{lstlisting}


\subsubsection{Prepare Rules database}
We are going to create sqlite3 database file with \texttt{rule} table for mined rules.

Create directory
\begin{lstlisting}
$ sudo mkdir -p /var/aida/rules
\end{lstlisting}

Install sqlite3 and create database file with required permissions.
\begin{lstlisting}
$ sudo apt-get -y install sqlite3
$ sudo sqlite3 /var/aida/rules/rule.db 'CREATE TABLE rule (id INTEGER PRIMARY KEY, inserted DATETIME DEFAULT CURRENT_TIMESTAMP, rule TEXT NOT NULL, support INTEGER, number_of_sequences INTEGER, confidence REAL, active INTEGER DEFAULT 0, comment TEXT, database TEXT, algorithm TEXT )'
$ sudo chmod 0666 /var/aida/rules/rule.db
\end{lstlisting}


\subsubsection{Install mining service}

Create mining user
\begin{lstlisting}
$ sudo useradd -M mining
\end{lstlisting}

Install mining binary
\begin{lstlisting}
$ sudo mkdir /opt/mining
$ cd /vagrant/services/mining
$ mvn clean
$ mvn package
$ sudo cp ./target/*jar-with-dependencies.jar /opt/mining/mining.jar
$ sudo chown mining:mining /opt/mining
\end{lstlisting}

Create systemd unit file for \texttt{mining} service
\begin{lstlisting}
$ sudo cp /vagrant/provision/ansible/roles/mining/templates/mining.service.j2 /etc/systemd/system/mining.service
\end{lstlisting}

Then replace following placeholders in newly created unit file
\begin{itemize}
\item \texttt{\{\{ mining\_user \}\}} with \texttt{mining} or other user you had chosen
\item \texttt{\{\{ mining\_dir \}\}} with \texttt{/opt/mining}
\item \texttt{\{\{ database\_path \}\}} with \texttt{/var/aida/rules/rule.db}
\end{itemize}

Create systemd timer for \texttt{mining} service
\begin{lstlisting}
$ sudo cp /vagrant/provision/ansible/roles/mining/templates/mining.timer.j2 /etc/systemd/system/mining.timer
\end{lstlisting}

Note that mining is a one-shot service. It will run the mining on all event currently available in Kafka and then finish.
You can trigger the mining by starting the service.
\begin{lstlisting}
$ sudo systemctl start mining
\end{lstlisting}

Then check it did exit successfully
\begin{lstlisting}
$ sudo systemctl status mining
\end{lstlisting}

Or you can check the logs
\begin{lstlisting}
$ sudo journalctl -u mining
\end{lstlisting}


\subsubsection{Install matching service}

Create matching user
\begin{lstlisting}
$ sudo useradd -M matching
\end{lstlisting}

Install matching binary
\begin{lstlisting}
$ sudo mkdir /opt/matching
$ cd /vagrant/services/matching
$ mvn clean package
$ sudo cp ./target/*jar-with-dependencies.jar /opt/matching/matching.jar
$ sudo chown -R matching:matching /opt/matching
\end{lstlisting}

Create systemd unit file for \texttt{matching} service
\begin{lstlisting}
$ sudo cp /vagrant/provision/ansible/roles/matching/templates/matching.service.j2 /etc/systemd/system/matching.service
\end{lstlisting}

Then replace following placeholders in newly created file
\begin{itemize}
\item \texttt{\{\{ matching\_user \}\}} with \texttt{matching} or other user you had chosen
\item \texttt{\{\{ matching\_dir \}\}} with \texttt{/opt/matching}
\item \texttt{\{\{ database\_path \}\}} with \texttt{/var/aida/rules/rule.db}
\end{itemize}

Run the \texttt{matching} service
\begin{lstlisting}
$ sudo systemctl start matching
\end{lstlisting}

Check that the service is running
\begin{lstlisting}
$ sudo systemctl status matching
\end{lstlisting}


\subsubsection{Install aida-output service}

\texttt{aida-output} will read predictions from Kafka and save them into file.

Create predictions file with permission of same user used for Kafka
\begin{lstlisting}
$ touch /vagrant/predictions.json
$ chown kafka:kafka /vagrant/predictions.json
\end{lstlisting}

Create systemd unit file for \texttt{aida-output} service
\begin{lstlisting}
$ sudo cp /vagrant/provision/ansible/roles/aida-output/templates/aida-output.service.j2 /etc/systemd/system/aida-output.service
\end{lstlisting}

Replace following placeholders in the newly created unit file.
\begin{itemize}
\item \texttt{\{\{ kafka\_user \}\}} with \texttt{kafka} or other user you had chosen to use while installing Kafka
\item \texttt{\{\{ kafka\_dir \}\}} with \texttt{/opt/kafka}
\item \texttt{\{\{ predictions\_file\_path \}\}} with \texttt{/vagrant/predictions.json}
\end{itemize}

Run the \texttt{aida-output} service
\begin{lstlisting}
$ sudo systemctl start aida-output
\end{lstlisting}

Check that the service is running
\begin{lstlisting}
$ sudo systemctl status aida-output
\end{lstlisting}


\subsubsection{Install restapi service}


\subsubsection{Install dashboard service}


\cleardoublepage

\section{Technical Documentation}

\subsection{Inputs and Outputs}

\subsection{Kafka}

\subsection{Spark}

\subsection{Sanitizer}

\subsection{Aggregation}

\subsection{Data Mining}

\subsection{Database}

\subsection{Rule Matching}

\subsection{Feedback}

\subsection{REST API}

    /componentsinfo/ - rozšířený stav komponent iABU (včetně výpisů systemd a posledních x řádků logu)

    /db/rules/YYYY-MM-DD/ - Zoznam pravidiel z daného dňa.
    /db/activerules/ - Zoznam aktívnych pravidiel.
    POST /db/activerules/id/ - Aktivuje pravidlo s daným ID.
    POST /db/inactiverules/id/ - Deaktivuje pravidlo s daným ID.
    POST /db/deleterules/id/ - Zmaže pravidlo s daným ID.
    POST /db/addrules/pravidlo/ - Vytvorí nové pravidlo.

    /enforcedatamining/ - vynutí spuštění data miningu nad současnou databází její smazání
    /reloadrulematching/ - vynutí restart Rule Matching komponenty spojený s načtením nové sady pravidel

Testovacia databáza: /var/www/restapi/sabu-test.db

\subsection{Dashboard}

\subsection{Ansbile and Vagrant}

\end{document}
